{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0239ea29",
   "metadata": {},
   "source": [
    "## **Build Smarter AI Apps: Empowering LLMs with LangChain & Gemini**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb07e13",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Use the core features of the LangChain framework, including prompt templates, chains, and agents, relative to enhancing LLM customization and output relevance.\n",
    "\n",
    "- Explore LangChain's modular approach, which supports dynamic adjustments to prompts and models without extensive code changes.\n",
    "\n",
    "- Enhance LLM applications by integrating retrieval-augmented generation (RAG) techniques with LangChain. You'll learn how integrating RAG enables greater accuracy and delivers improved contextually-aware responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dfbfd5",
   "metadata": {},
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540e6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Ensure the core LangChain ecosystem is up to date\n",
    "!pip install -U --no-cache-dir langchain langchain-community langchain-google-genai\n",
    "\n",
    "# Essential utilities for RAG (Retrieval Augmented Generation)\n",
    "!pip install -U pypdf chromadb langchainhub\n",
    "\n",
    "# Specifically for Gemini's multimodal and generative features\n",
    "!pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae8b2f",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A large language model (LLM) serves as the interface for the AI's capabilities. The LLM processes plain text input and generates text output, forming the core functionality needed to complete various tasks. When integrated with LangChain, the LLM becomes a powerful tool, providing the foundational structure necessary for building and deploying sophisticated AI applications.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `ModelInference` module from `Gemini`. To configure your own API key, run the code cell below with your key in the `GEMINI_API_KEY` field of `credentials`. **DO NOT** uncomment the `GEMINI_API_KEY` field if you aren't running locally, it will causes errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17802ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kungdevz/workspaces/ai-app-langchain-gemini/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3483/3230939870.py:3: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching active models...\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-pro\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-exp\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-001\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-exp-image-generation\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-lite-001\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-lite\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-lite-preview-02-05\n",
      "ðŸ‘‰ Use this ID: gemini-2.0-flash-lite-preview\n",
      "ðŸ‘‰ Use this ID: gemini-exp-1206\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-preview-tts\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-pro-preview-tts\n",
      "ðŸ‘‰ Use this ID: gemma-3-1b-it\n",
      "ðŸ‘‰ Use this ID: gemma-3-4b-it\n",
      "ðŸ‘‰ Use this ID: gemma-3-12b-it\n",
      "ðŸ‘‰ Use this ID: gemma-3-27b-it\n",
      "ðŸ‘‰ Use this ID: gemma-3n-e4b-it\n",
      "ðŸ‘‰ Use this ID: gemma-3n-e2b-it\n",
      "ðŸ‘‰ Use this ID: gemini-flash-latest\n",
      "ðŸ‘‰ Use this ID: gemini-flash-lite-latest\n",
      "ðŸ‘‰ Use this ID: gemini-pro-latest\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-lite\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-image-preview\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-image\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-preview-09-2025\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-flash-lite-preview-09-2025\n",
      "ðŸ‘‰ Use this ID: gemini-3-pro-preview\n",
      "ðŸ‘‰ Use this ID: gemini-3-flash-preview\n",
      "ðŸ‘‰ Use this ID: gemini-3-pro-image-preview\n",
      "ðŸ‘‰ Use this ID: nano-banana-pro-preview\n",
      "ðŸ‘‰ Use this ID: gemini-robotics-er-1.5-preview\n",
      "ðŸ‘‰ Use this ID: gemini-2.5-computer-use-preview-10-2025\n",
      "ðŸ‘‰ Use this ID: deep-research-pro-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Google Gemini Implementation ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# --- LangChain Core (The \"Orchestration\" Layer - Stays the same!) ---\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Note: LLMChain is being deprecated in favor of LCEL (LangChain Expression Language)\n",
    "# I will show you how to use the modern pipe (|) syntax below.\n",
    "\n",
    "if \"GEMINI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter your Gemini API Key: \")\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "print(\"Fetching active models...\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        # This will print the exact string needed for langchain\n",
    "        print(f\"ðŸ‘‰ Use this ID: {m.name.replace('models/', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1049705",
   "metadata": {},
   "source": [
    "### Your First Gemini Chain (LCEL Style)\n",
    "Instead of using the older LLMChain, modern AI engineering uses LCEL (LangChain Expression Language). It is more readable and easier to debug.\n",
    "\n",
    "Let's initialize the model and create a simple chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99261a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how I'd explain vector embeddings to a junior developer:\n",
      "\n",
      "*   **Think of them as \"meaningful coordinates\" for data:** Instead of representing text, images, or other data as raw characters or pixels, vector embeddings translate them into lists of numbers (vectors) in a high-dimensional space. The key is that similar pieces of data are placed *close together* in this space, while dissimilar ones are far apart.\n",
      "\n",
      "*   **They capture semantic relationships:** These numerical representations aren't random. They're learned by AI models that understand context, relationships, and nuances. For example, the embedding for \"king\" will be mathematically closer to \"queen\" than it is to \"banana,\" because the model has learned the semantic connection between royalty.\n",
      "\n",
      "*   **They unlock powerful AI capabilities:** By converting complex data into these numerical vectors, we can perform sophisticated tasks like finding similar documents, recommending products, classifying text, or even answering questions (when combined with language models), all through simple mathematical operations like measuring distances between vectors.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the Model (The \"Brain\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    temperature=0.7,\n",
    "    convert_system_message_to_human=True # Helpful for some Gemini versions\n",
    ")\n",
    "\n",
    "# 2. Define the Prompt (The \"Instructions\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a senior AI Engineer. Explain {topic} to a junior developer in 3 bullet points.\"\n",
    ")\n",
    "\n",
    "# 3. Create the Chain using the Pipe Operator (|)\n",
    "# This flows: Input -> Prompt -> LLM -> String Output\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. Execute\n",
    "response = chain.invoke({\"topic\": \"Vector Embeddings\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
